To Deploy it in EC2

http://spark.apache.org/docs/1.0.1/ec2-scripts.html

to start cluster command used - Go to spark ec2 folder and run below command 
Deploy - This is the name of the keyvalue pair
.bashrc - This has the access key id and access key token for authentication
Deply.pem has the ssh for the keyvalue pair created
us-west-2 - Region in which amazon clusters are deployed - Default is east ours is west-2 which is oregan 
1 - Number of slave nodes
instance_type - m1.medium ie medium size cluster
haha - name of the master node and slave node

Run - chmod 600 on Deploy.pem file

./spark-ec2 --region=us-west-2 --instance-type=m1.medium -k Deploy -i ../../Deploy.pem -s 1 launch haha 

To ssh into cluster 

./spark-ec2 -k Deploy -i ../../Deploy.pem login haha --region=us-west-2

After SSH into cluster run
wget https://s3-us-west-2.amazonaws.com/testimagesvectors/Output.txt
to get data into your cluster

put the data into hdfs by going into the ephemeral-hdfs bin and doing hadoop fs -put input.txt TargetHDFSDir/inp.txt

go to /root/ephemeral-hdfs/conf/core-site.xml to get to the element fs.default.name whose value gives the way you can access the files in hdfs
example - hdfs://ec2-54-68-181-89.us-west-2.compute.amazonaws.com:9000/user/root/input/inp.txt

use such path for bpth reading and writing




