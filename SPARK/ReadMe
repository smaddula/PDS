To Deploy it in EC2

http://spark.apache.org/docs/1.0.1/ec2-scripts.html

to start cluster command used - Go to spark ec2 folder and run below command 
Deploy - This is the name of the keyvalue pair
.bashrc - This has the access key id and access key token for authentication
Deply.pem has the ssh for the keyvalue pair created
us-west-2 - Region in which amazon clusters are deployed - Default is east ours is west-2 which is oregan 
1 - Number of slave nodes
instance_type - m1.medium ie medium size cluster
haha - name of the master node and slave node

Run - chmod 600 on Deploy.pem file

./spark-ec2 --region=us-west-2 --instance-type=m1.medium -k Deploy -i ../../Deploy.pem -s 1 launch haha 

To ssh into cluster 

./spark-ec2 -k Deploy -i ../../Deploy.pem login haha --region=us-west-2

After SSH into cluster run
wget https://s3-us-west-2.amazonaws.com/testimagesvectors/Output.txt
to get data into your cluster - This is not required if you want to read the data directly from s3 and write in s3 as well 
                               -using input output folder paths like below

inputfile = "s3n://AKIAJCR562YH2YP2CPXQ:IKPvgGvqhlND26XX8WseMQGHTxgBpWx9uFGCF3po@testimagesvectors/Output.txt"
OutputDir = "s3n://AKIAJCR562YH2YP2CPXQ:IKPvgGvqhlND26XX8WseMQGHTxgBpWx9uFGCF3po@testinghahaoutput/testing"

Its of the format  s3n://AccessKeyID:SECRET@BUCKET/Path

use git clone https://github.com/smaddula/PDS.git to get code into the server

#no longer required if input output is taken directly from S3 like above
go to /root/ephemeral-hdfs/conf/core-site.xml to get to the element fs.default.name whose value gives the way you can access the files in hdfs
example - hdfs://ec2-54-68-181-89.us-west-2.compute.amazonaws.com:9000/user/root/input/inp.txt


